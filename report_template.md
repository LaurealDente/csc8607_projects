# Rapport de projet ‚Äî CSC8607 : Introduction au Deep Learning

> **Consignes g√©n√©rales**
> - Tenez-vous au **format** et √† l‚Äô**ordre** des sections ci-dessous.
> - Int√©grez des **captures d‚Äô√©cran TensorBoard** lisibles (loss, m√©triques, LR finder, comparaisons).
> - Les chemins et noms de fichiers **doivent** correspondre √† la structure du d√©p√¥t mod√®le (ex. `runs/`, `artifacts/best.ckpt`, `configs/config.yaml`).
> - R√©pondez aux questions **num√©rot√©es** (D1‚ÄìD11, M0‚ÄìM9, etc.) directement dans les sections pr√©vues.

---

## 0) Informations g√©n√©rales

- **√âtudiant¬∑e** : _Lauret, Alexandre_
- **Projet** : _Projet 23 (tiny imagenet √ó Blocs r√©siduels + Dropout2d dans les bloc)_
- **D√©p√¥t Git** : _URL publique_
- **Environnement** : `python == 3.10.18`, `torch == 2.5.1`, `cuda == 12.0.140`  
- **Commandes utilis√©es** :
  - Entra√Ænement : `python -m src.train --config configs/config.yaml`
  - LR finder : `python -m src.lr_finder --config configs/config.yaml`
  - Grid search : `python -m src.grid_search --config configs/config.yaml`
  - √âvaluation : `python -m src.evaluate --config configs/config.yaml --checkpoint artifacts/best.ckpt`

---

## 1) Donn√©es

### 1.1 Description du dataset
- **Source** (lien) : https://huggingface.co/datasets/zh-plus/tiny-imagenet
- **Type d‚Äôentr√©e** (image / texte / audio / s√©ries) : images
- **T√¢che** (multiclasses, multi-label, r√©gression) : classification multiclasses
- **Dimensions d‚Äôentr√©e attendues** (`meta["input_shape"]`) : 3x64x64
- **Nombre de classes** (`meta["num_classes"]`) : 200

**D1.** Quel dataset utilisez-vous ? D‚Äôo√π provient-il et quel est son format (dimensions, type d‚Äôentr√©e) ?

Le dataset que je vais utiliser est zh-plus/tiny-imagenet stock√© sur HuggingFace Datasets.
Celui-ci est compos√© de 110,000 lignes et de 2 colonnes. La premi√®re colonne, nomm√©e "image", sont les images. La deuxi√®me colonne, nomm√©e label, est compos√©e des labels de chacune de ces images.


### 1.2 Splits et statistiques

| Split | #Exemples | Particularit√©s (d√©s√©quilibre, longueur moyenne, etc.) |
|------:|----------:|--------------------------------------------------------|
| Train |           |                                                        |
| Val   |           |                                                        |
| Test  |           |                                                        |

**D2.** Donnez la taille de chaque split et le nombre de classes. 

Le dataset est compos√© de deux split, le premier est celui d'entra√Ænement (train), le deuxi√®me est le split de validation (valid). Dans chacun des deux splits, nous pouvons trouver l'ensemble des 200 classes.

**D3.** Si vous avez cr√©√© un split (ex. validation), expliquez **comment** (stratification, ratio, seed).

Le dataset proposait d√©j√† deux datasets un train et un de validation. J'ai cr√©√© un split de test √† partir du dataset de train.
Pour cela, j'ai utilis√© un ratio de 0.1 afin d'avoir un m√™me nombre de valeurs entre le split de test et de validation qui ont 1000 lignes.
Le split de train a alors 9000 lignes restantes pour l'entra√Ænement.

Pour la stratification, j'ai cibl√© les valeurs de la colonne label afin d'√©quilibrer les trois splits. Ceci permettra un apprentissage de qualit√© et une meilleure √©valuation du mod√®le.

La seed utilis√©e est stock√©e dans le fichier de configuration la variable par d√©faut √©tait 42, valeur que j'ai utilis√©e.


**D4.** Donnez la **distribution des classes** (graphique ou tableau) et commentez en 2‚Äì3 lignes l‚Äôimpact potentiel sur l‚Äôentra√Ænement.  

Gr√¢ce √† la r√©partition √©gale de chaque classe (m√™me nombre de label), nous allons pouvoir entra√Æner chacune des classes √©quitablement.
Cela permettra d'avoir une meilleure pr√©cision sur la matrice de confusion.


**D5.** Mentionnez toute particularit√© d√©tect√©e (tailles vari√©es, longueurs variables, multi-labels, etc.).

Une particularit√© est d√©tect√©e apr√®s traitement. Les tailles des images sont √©gales avec un format de 64x64 avec 3 channel (RGB). Sauf 2% des images qui n'ont qu'un seul canal (L).
Les labels sont tous des entiers entre 0 et 199, il n'y a aucun multi labels.


### 1.3 Pr√©traitements (preprocessing) ‚Äî _appliqu√©s √† train/val/test_

Listez pr√©cis√©ment les op√©rations et param√®tres (valeurs **fixes**) :

- Vision : resize = , center-crop = None, normalize = (mean=[0.4802, 0.4480, 0.3974], std=[0.2765, 0.2689, 0.2816])

**D6.** Quels **pr√©traitements** avez-vous appliqu√©s (op√©rations + **param√®tres exacts**) et **pourquoi** ?  

Les pr√©traitements appliqu√©s au dataset sont une convertion de toutes les images en format RGB car 2% sont stock√©es en format L.
La deuxi√®me transformation est la conversion en tensor. Les valeurs des tensors sont comprises entre 0 et 1, le calcul de la moyenne et de l'√©cart type permet de normaliser ces tensors. La normalisation va permettre d'avoir des entr√©es sur des √©chelles similaires. Elle acc√©l√®re la convergence du mod√®le et stabilise les calculs num√©riques.

**D7.** Les pr√©traitements diff√®rent-ils entre train/val/test (ils ne devraient pas, sauf recadrage non al√©atoire en val/test) ?

Les pr√©traitrements sont similaires entre les trois datasets, la normalisation est calcul√©e sur les moyennes et les ecarts types du datset train afin de ne pas faire de data leaking.

### 1.4 Augmentation de donn√©es ‚Äî _train uniquement_

- Liste des **augmentations** (op√©rations + **param√®tres** et **probabilit√©s**) :
  - Flip horizontal p=0.5, RandomResizedCrop size=64x64, scale=80-100%, ratio=1 

**D8.** Quelles **augmentations** avez-vous appliqu√©es (param√®tres pr√©cis) et **pourquoi** ?  

Afin d'augmenter la volum√©trie des donn√©es d'entra√Ænement, j'ai utilis√© le random flip, le random crop et le color jitter.
Le random flip est d√©fini √† 0.5 dans le code pour retourner al√©atoirement l'image lors de l'entra√Ænement avec une probabilit√© d'1/2. Il est possible de l'utiliser sur les images de notre dataset car le mod√®le ne se focalise pas sur des textes ou √©quivalents n√©cessitant un sens d√©fini.
Le random crop permet de ne prendre qu'une partie de l'image de base pour entra√Æner le mod√®le sur des centrages diff√©rents. Ici les images sont crop√©es au maximum de 20% qui va permettre d'√©viter la perte compl√®te de l'objet examin√©.
Le color jitter permet d'influencer les param√®tres de la photo repr√©sentant des contextes photographiques pouvant √™tre changeants. La luminosit√©, le contrast, la saturation et la teinte de mani√®re al√©atoire entre x1.2 et x0.8 car tous les param√®tres sont r√©gl√©s √† 0.2. 


**D9.** Les augmentations **conservent-elles les labels** ? Justifiez pour chaque transformation retenue.

Les labels sont conserv√©s pour chacune des augmentations, en effet ces modifications s'appliqueront sur l'image r√©cup√©r√©es de base lors du DataLoader. Au cours de l'entra√Ænement, DataLoader appel _getitem() qui modifiera les images sur la base des probabilit√©s cit√©es auparavant.
Le randomhorizontalfip retourne une image, il ne modifie pas le label hormis lorsque ce sont des textes o√π nous perdons de l'information. Ici il est applicable sans perte la classification ne se repose pas sur des images avec des textes.
Le color jitter modifie les param√®tres de l'image mais celle-ci sont toujours autant reconnaissables, elles gardent leurs caract√©ristiques principales. Les labels sont gard√©s.
Le random crop pourrait entra√Æner une perte d'information sur l'image et faire perdre la logique image-label, cependant ici le crop est de 80 √† 100% ce qui garde la plus grande partie de l'image et √©vite de perdre totalement l'objet √©tudi√©.

### 1.5 Sanity-checks

- **Exemples** apr√®s preprocessing/augmentation (ins√©rer 2‚Äì3 images/spectrogrammes) :

> _Ins√©rer ici 2‚Äì3 captures illustrant les donn√©es apr√®s transformation._
![alt text](images/check_img_77669_pole.png)
![alt text](images/check_img_31474_chain.png)
![alt text](<images/check_img_70739_dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk.png>)



**D10.** Montrez 2‚Äì3 exemples et commentez bri√®vement. 
On voit un changement des couleurs (colorjitter) un recadrage (randomcrop) et un horizontalflip apr√®s augmentation des images.


**D11.** Donnez la **forme exacte** d‚Äôun batch train (ex. `(batch, C, H, W)` ou `(batch, seq_len)`), et v√©rifiez la coh√©rence avec `meta["input_shape"]`.
[32, 3, 64, 64]



---

## 2) Mod√®le

### 2.1 Baselines

**M0.**
- **Classe majoritaire** ‚Äî M√©trique : `Accuracy` ‚Üí score = `0.5%`
- **Pr√©diction al√©atoire uniforme** ‚Äî M√©trique : `Accuracy` ‚Üí score = `0.5%`  
_Commentez en 2 lignes ce que ces chiffres impliquent._
La classe majoritaire serait un mod√®le qui pr√©dit toujours une seule classe, un seul label, celui de la classe majoritaire. Le score est de 0.5% car la classe majoritaire est compos√©e de 500 donn√©es sur 100,000 existantes (500/100,000 = 0.005).

La pr√©diction al√©atoire uniforme a autant de chance de pr√©dire chacune des classes. Sachant que toutes les classes ont la m√™me probabilit√© de 0.005 alors la m√™me accuracy est calcul√©e que pour la m√©thode la classe majoritaire.

### 2.2 Architecture impl√©ment√©e

- **Description couche par couche** (ordre exact, tailles, activations, normalisations, poolings, r√©siduels, etc.) :

Ici, l'exemple d'un batch de 32 est pris. Il se peut que l'utilisation soit ensuite faite sur un autre nombre de batch selon la configuration stock√©e dans config.yaml et les performances que cela entra√Æne.

  - Input : 
      Conv2d, BatchNorm2d(option), ReLU (ou d√©riv√©e)
      Entr√©e (32, 3, 64, 64) Sortie (32, 64, 64, 64)
      La fonction d'activation est ReLU par d√©faut, l'option de choisir avec GeLU, SeLU, LeakyReLU et ELU a √©t√© ajout√©e.
      Aucun Pooling utilis√© dans ce ResNet
  - Stage 1 (r√©p√©ter N‚ÇÅ fois) : 
      Conv2d, BatchNorm2d(option), ReLU(ou d√©riv√©e), Dropout2d, Conv2d, BatchNorm2d(option)
      Entr√©e (32, 64, 64, 64) Sortie (32, 64, 64, 64)
  - Stage 2 (r√©p√©ter N‚ÇÇ fois) : 
      Conv2d, BatchNorm2d(option), ReLU(ou d√©riv√©e), Dropout2d, Conv2d, BatchNorm2d(option)
      Entr√©e (32, 64, 64, 64) Sortie (32, 128, 32, 32)
  - Stage 3 (r√©p√©ter N‚ÇÉ fois) : 
      Conv2d, BatchNorm2d(option), ReLU(ou d√©riv√©e), Dropout2d, Conv2d, BatchNorm2d(option)
      Entr√©e (32, 128, 32, 32) Sortie (32, 256, 16, 16)
  - T√™te (GAP / lin√©aire) ‚Üí logits (dimension = nb classes)
      AdaptativeAvgPool2d, Flatten, Linear
      Entr√©e (32, 256, 16, 16) Sortie (32, 200) (batch_size, num_classes)
      Pas de fonction d'ac

- **Loss function** :
  - Multi-classe : CrossEntropyLoss

- **Sortie du mod√®le** : forme = __(32, 200)__ (ou __(batch_size, num_attributes)__)

- **Nombre total de param√®tres** : `2 805 448` ou `4 584 776` en fonction du nombre de blocs r√©siduels

**M1.** D√©crivez l‚Äô**architecture** compl√®te et donnez le **nombre total de param√®tres**.  
Expliquez le r√¥le des **2 hyperparam√®tres sp√©cifiques au mod√®le** (ceux impos√©s par votre sujet).

Il existe 3 composantes principales √† ce r√©seau, la phase d'input, l'empilement des blocs r√©siduels et enfin la t√™te.
La phase d'input permet de pr√©parer les donn√©es d'entr√©e √† nos blocs r√©siduels en changeant le nombre de channel.
La phase de bloc r√©siduel permet de reconna√Ætre les patternes qui peuvent d√©finir un objet, une classe d'image.
La derni√®re phase, la t√™te permet de transformer ces patternes en pr√©diction en les structurant en une sortie compr√©hensible (nombre de batchs, num√©ro label).

Deux hyperparam√®tres sp√©cifiques existent pour ce mod√®le. Le nombre de blocs r√©siduels et le taux de drop out.
Le nombre de blocs (2,2,2 ou 3,3,3) permet de r√©guler la profondeur du r√©seau, lorsque la profondeur est plus grande, le risque d'overfitting l'est aussi.
Le taux de dropout r√©gule ce probl√®me d'overfitting en donnant une probabilit√© de d√©sactivation de neurones. Cela rend le mod√®le plus robuste. (0.1 ou 0.3).
Ces deux hyperparam√®tres vont parfaitement de paire.

### 2.3 Perte initiale & premier batch

- **Loss initiale attendue** (multi-classe) ‚âà `-log(1/num_classes)` = 5.298
- **Observ√©e sur un batch** : `5.2958`
- **V√©rification** : backward OK, gradients ‚â† 0

**M2.** Donnez la **loss initiale** observ√©e et dites si elle est coh√©rente. Indiquez la forme du batch et la forme de sortie du mod√®le.

La loss initiale obtenue est de 5.2958, tr√®s proche de la loss th√©orique, cela signifie que le mod√®le est coh√©rent. Le batch est de forme (32,3,64,64) et la sortie du mod√®le de forme (32, 200).

---

## 3) Overfit ¬´ petit √©chantillon ¬ª

- **Sous-ensemble train** : `N = ____` exemples
- **Hyperparam√®tres mod√®le utilis√©s** (les 2 √† r√©gler) : `dropout = 0.1`, `blocs r√©siduels = [3,3,3]`
- **Optimisation** : LR = `0.001`, weight decay = `0` (0 ou tr√®s faible recommand√©)
- **Nombre d‚Äô√©poques** : `100`

> _Ins√©rer capture TensorBoard : `train/loss` montrant la descente vers ~0._
![alt text](images/image.png)
Le mod√®le apprend bien sur le petit ensemble.

**M3.** Donnez la **taille du sous-ensemble**, les **hyperparam√®tres** du mod√®le utilis√©s, et la **courbe train/loss** (capture). Expliquez ce qui prouve l‚Äôoverfit.

Le test a √©t√© men√© sur un sous-ensemble de 64 exemples avec les hyperparam√®tres de mod√®le B=(3,3,3) et dropout=0.1 afin de maximiser les potentiels d'overfitting. La courbe train/loss (voir capture ci-dessus) montre que la perte d'entra√Ænement diminue de mani√®re drastique pour tendre vers z√©ro d√®s 40 √©poques.

Ce comportement prouve l'overfitting car il d√©montre que le mod√®le a une capacit√© suffisante pour m√©moriser parfaitement ce petit jeu de donn√©es. S'il n'arrivait pas √† faire chuter la perte, cela indiquerait un probl√®me dans l'architecture ou le pipeline d'entra√Ænement. Le succ√®s de ce test valide donc la capacit√© d'apprentissage de notre mod√®le.

---

## 4) LR finder

- **M√©thode** : balayage LR (log-scale), quelques it√©rations, log `(lr, loss)`
- **Fen√™tre stable retenue** : `7.0e-08 ‚Üí 9.9e-04`
- **Choix pour la suite** :
  - **LR** = `0.0001`
  - **Weight decay** = `1e-05` (valeurs classiques : 1e-5, 1e-4)

> _Ins√©rer capture TensorBoard : courbe LR ‚Üí loss._

**M4.** Justifiez en 2‚Äì3 phrases le choix du **LR** et du **weight decay**.
Classement des combinaisons (de la meilleure √† la moins bonne):

    Learning Rate  Weight Decay      Loss
0         0.00010       0.00001  5.058476
1         0.00010       0.00100  5.061198
2         0.00010       0.00000  5.073034
3         0.00005       0.00001  5.124333
4         0.00005       0.00100  5.128525
5         0.00010       0.00010  5.130843
6         0.00005       0.00000  5.145882
7         0.00050       0.00100  5.148840
8         0.00005       0.00010  5.152627
9         0.00050       0.00000  5.154968
10        0.00050       0.00010  5.194970
11        0.00100       0.00010  5.198978
12        0.00100       0.00100  5.210259
13        0.00050       0.00001  5.216026
14        0.01000       0.00001  5.254538
15        0.01000       0.00010  5.269663
16        0.00100       0.00000  5.269957
17        0.00500       0.00001  5.271437
18        0.01000       0.00100  5.281708
19        0.01000       0.00000  5.299198
20        0.00500       0.00100  5.314303
21        0.00500       0.00000  5.319302
22        0.00001       0.00000  5.327222
23        0.00001       0.00100  5.328353
24        0.00001       0.00010  5.329051
25        0.00001       0.00001  5.334647
26        0.00500       0.00010  5.343188
27        0.00100       0.00001  5.343339
---

## 5) Mini grid search (rapide)

- **Grilles** :
  - LR : `{9.9e-05}`
  - Weight decay : `{1e-5}`
  - Hyperparam√®tre mod√®le A : `{(2,2,2), (3,3,3)}`
  - Hyperparam√®tre mod√®le B : `{0.1, 0.3}`

- **Dur√©e des runs** : `5` √©poques par run (1‚Äì5 selon dataset), m√™me seed

==================================================
R√âSULTATS DE LA GRID SEARCH
==================================================
üèÜ Meilleure accuracy de validation : 1.50%
Hyperparam√®tres correspondants :
  - lr: 9.9e-05
  - weight_decay: 1e-07
  - dropout_p: 0.3
  - block_config: [2, 2, 2]
==================================================

Sur un ensemble de train de 10,000 et un ensemble de test de 2,000

================================================================================
TABLEAU R√âCAPITULATIF DE LA GRID SEARCH
================================================================================
| Run (nom explicite)                                                  |      LR |    WD | Hyp-A (block_config)   |   Hyp-B (dropout_p) |   Val metric (nom=Accuracy (%)) |   Val loss | Notes   |
|:---------------------------------------------------------------------|--------:|------:|:-----------------------|--------------------:|--------------------------------:|-----------:|:--------|
| run_lr=9.9e-05_weight_decay=1e-05_dropout_p=0.1_block_config=[2-2-2] | 9.9e-05 | 1e-05 | [2, 2, 2]              |                 0.1 |                            1.3  |     6.4148 |         |
| run_lr=9.9e-05_weight_decay=1e-05_dropout_p=0.1_block_config=[3-3-3] | 9.9e-05 | 1e-05 | [3, 3, 3]              |                 0.1 |                            2.75 |     5.3295 |         |
| run_lr=9.9e-05_weight_decay=1e-05_dropout_p=0.3_block_config=[2-2-2] | 9.9e-05 | 1e-05 | [2, 2, 2]              |                 0.3 |                            2.3  |     6.2908 |         |
| run_lr=9.9e-05_weight_decay=1e-05_dropout_p=0.3_block_config=[3-3-3] | 9.9e-05 | 1e-05 | [3, 3, 3]              |                 0.3 |                            1.35 |     5.7039 |         |



> _Ins√©rer capture TensorBoard (onglet HParams/Scalars) ou tableau r√©capitulatif._

**M5.** Pr√©sentez la **meilleure combinaison** (selon validation) et commentez l‚Äôeffet des **2 hyperparam√®tres de mod√®le** sur les courbes (stabilit√©, vitesse, overfit).



---

## 6) Entra√Ænement complet (10‚Äì20 √©poques, sans scheduler)

- **Configuration finale** :
  - LR = `_____`
  - Weight decay = `_____`
  - Hyperparam√®tre mod√®le A = `_____`
  - Hyperparam√®tre mod√®le B = `_____`
  - Batch size = `_____`
  - √âpoques = `_____` (10‚Äì20)
- **Checkpoint** : `artifacts/best.ckpt` (selon meilleure m√©trique val)

> _Ins√©rer captures TensorBoard :_
> - `train/loss`, `val/loss`
> - `val/accuracy` **ou** `val/f1` (classification)

**M6.** Montrez les **courbes train/val** (loss + m√©trique). Interpr√©tez : sous-apprentissage / sur-apprentissage / stabilit√© d‚Äôentra√Ænement.

---

## 7) Comparaisons de courbes (analyse)

> _Superposez plusieurs runs dans TensorBoard et ins√©rez 2‚Äì3 captures :_

- **Variation du LR** (impact au d√©but d‚Äôentra√Ænement)
- **Variation du weight decay** (√©cart train/val, r√©gularisation)
- **Variation des 2 hyperparam√®tres de mod√®le** (convergence, plateau, surcapacit√©)

**M7.** Trois **comparaisons** comment√©es (une phrase chacune) : LR, weight decay, hyperparam√®tres mod√®le ‚Äî ce que vous attendiez vs. ce que vous observez.

---

## 8) It√©ration suppl√©mentaire (si temps)

- **Changement(s)** : `_____` (resserrage de grille, nouvelle valeur d‚Äôun hyperparam√®tre, etc.)
- **R√©sultat** : `_____` (val metric, tendances des courbes)

**M8.** D√©crivez cette it√©ration, la motivation et le r√©sultat.

---

## 9) √âvaluation finale (test)

- **Checkpoint √©valu√©** : `artifacts/best.ckpt`
- **M√©triques test** :
  - Metric principale (nom = `_____`) : `_____`
  - Metric(s) secondaire(s) : `_____`

**M9.** Donnez les **r√©sultats test** et comparez-les √† la validation (√©cart raisonnable ? surapprentissage probable ?).

---

## 10) Limites, erreurs & bug diary (court)

- **Limites connues** (donn√©es, compute, mod√®le) :
- **Erreurs rencontr√©es** (shape mismatch, divergence, NaN‚Ä¶) et **solutions** :
- **Id√©es ¬´ si plus de temps/compute ¬ª** (une phrase) :

---

## 11) Reproductibilit√©

- **Seed** : `_____`
- **Config utilis√©e** : joindre un extrait de `configs/config.yaml` (sections pertinentes)
- **Commandes exactes** :

```bash
# Exemple (remplacer par vos commandes effectives)
python -m src.train --config configs/config.yaml --max_epochs 15
python -m src.evaluate --config configs/config.yaml --checkpoint artifacts/best.ckpt
````

* **Artifacts requis pr√©sents** :

  * [ ] `runs/` (runs utiles uniquement)
  * [ ] `artifacts/best.ckpt`
  * [ ] `configs/config.yaml` align√© avec la meilleure config

---

## 12) R√©f√©rences (courtes)

* PyTorch docs des modules utilis√©s (Conv2d, BatchNorm, ReLU, LSTM/GRU, transforms, etc.).
* Lien dataset officiel (et/ou HuggingFace/torchvision/torchaudio).
* Toute ressource externe substantielle (une ligne par r√©f√©rence).


